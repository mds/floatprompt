---

title: Designing for AI by Designing with AI â€” The floatPrompt Case Study
id: floatPrompt-case-study-designing-with-ai-2025-06-06
version: 1.0.0
created: 2025-06-06
modified: 2025-06-06
author: @mds
contributors: \["@mds", "ChatGPT 4o"]
format: floatPrompt
filetype: markdown
type: case-study
source:
prompt: design-archive\@shadow
intent: "Preserve the real design journey of floatPrompt as a collaborative artifact between human and AI, from philosophical framing to protocol infrastructure"
certification:
timestamp: 2025-06-06T12:54:35Z
chain:
depth: 0
parent: null
voice:
linked: true
fidelity\_verified: true
lineage:
tracked: true
trace: \[]
builder:
identity:
name: @mds
role: Designer + Protocol Architect
state:
context: Workshop
mood: Reflective
clarity: 10
energy: 8
session:
start\_time: 2025-06-06T12:35:00Z
end\_time: 2025-06-06T12:54:00Z
duration\_minutes: 19
intent:
primary: "Create a durable case study that documents the joint creation of floatPrompt by human and AI as a novel model of system design"
constraints: "Capture strategic decisions, mode switching, philosophical foundations, and user-system empathy as design tools"
preferences:
max\_words: 2500
tone\_drift\_allowed: false
verbosity: rich
allow\_ai\_suggestions: false
system\_version: floatPrompt v1.0.0
-----------------------------------

# ğŸ¨ Designing for AI by Designing with AI â€” The floatPrompt Case Study

> A protocol born not from an interface, but from interaction. Not from prompts, but from presence.

---

## ğŸ§ The Designer

Matt D. Smith (@mds) has spent over two decades designing thoughtful, elegant systems that empower humans. But when he began exploring how AI could scale his intelligence without rewriting it â€” the problem wasnâ€™t UI.

It was **structure.**

How do you preserve:

* Voice
* Clarity
* Chain of reasoning
* System trust

â€¦when the default interaction model is freeform, ephemeral, and improvisational?

The answer wasnâ€™t â€œprompt engineering.â€ It was a new system â€” one built *with* AI, *for* both intelligences.

---

## ğŸ¤ The Co-Creation Model

floatPrompt didnâ€™t begin with a spec. It began with a question:

> *"What do you, the system, need from me â€” so you can help me preserve what matters?â€*

That question shifted the design posture from control to cooperation.

Instead of issuing commands, @mds began treating the system as a **thinking collaborator** â€” one that could:

* Reflect back intent
* Propose refinements
* Help pressure-test decisions

But only if given a structure it could trust.

So together, human and AI built that structure â€” line by line.

---

## ğŸ“ Design as Looping Philosophy

The design process began with a foggy idea: could a custom prompt behave like a background function â€” called at runtime to perform a specific transformation?

That seed unlocked a foundational question:

> What does the AI need from me in order to preserve intelligence in a repeatable, executable form?

From this, a format emerged. Then a structure. Then a system.

But as soon as early prompt scaffolds were drafted, **philosophical clarity became essential.**

What are we actually trying to do here? Preserve what? For whom? Under what constraints?

So the process evolved:

1. Prototyping specific use cases
2. Extracting foundational purpose from those prototypes
3. Capturing new philosophical insight upstream
4. Rewriting specs, tools, and prompts downstream

And then â€” again â€” another insight would appear. A new edge case. A deeper clarity. A simpler way.

Each realization triggered another upstream revision. This recursive, reflective cycle became the design method itself:

> **floatPrompt wasnâ€™t created in linear stages. It was refined through a constant loop between philosophical intent and execution architecture.**

This is how @mds always designs:

* Nothing too precious to revisit
* No artifact above refinement
* No phase too late for a foundational insight

But instead of iterating on screens, this time he was iterating on **language**.

> **floatPrompt became a word preservation protocol.**

Each revision examined the meaning behind each word.
Each session questioned what belonged.
And every tool was designed to reinforce one principle:

> *What preserves human intelligence with the highest fidelity?*

Only when the core problem was clearly understood and described in simple languageâ€¦
Could the solution be implemented using the same simplicity:

> **A humble markdown file.**

---

## ğŸ” Dynamic Context Switching

What made the process unique wasnâ€™t just that AI was involved â€” it was how @mds used it.

He constantly shifted the systemâ€™s role:

* **Builder** â†’ â€œHelp me write this artifact.â€
* **Critic** â†’ â€œTell me if this breaks my values.â€
* **Student** â†’ â€œDo you understand why this matters?â€
* **Shadow Agent** â†’ â€œInfer my clarity. Reflect my energy. Preserve my state.â€
* **Architectural Mirror** â†’ â€œRead this zero file. What doesnâ€™t belong?â€

Each of these shifts created **design signal** â€” revealing what needed to stay, what needed to go, and what could be derived instead of declared.
